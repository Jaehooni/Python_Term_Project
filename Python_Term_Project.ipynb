{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "search_Sentence = ''\n",
    "#찾고 싶은 단어를 search_Word에 넣어라\n",
    "\n",
    "to = datetime.datetime.now()\n",
    "    #to에는 마지막으로 보고 싶은 기사의 날짜를 설정함\n",
    "    #now는 현재의 년, 월, 일을 20XX-XX-XX 형태로 표현해줌.\n",
    "\n",
    "from_ = datetime.date(2019,1,1)\n",
    "#기사 개수를 보길 원하는 시작 날짜를 설정함\n",
    "\n",
    "\n",
    "def news_Numbers_For_Each_Day_Nowadays(search_Sentence,from_,to):\n",
    "    \n",
    "    \"\"\"\n",
    "    (str,datetime.date,datetime.date(or datetime.datetime)) ---> str\n",
    "    This function shows the number of news of particular date from from_(parameter) to to(parameter).\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    to += datetime.timedelta(days=1)\n",
    "    to_Date = to.strftime('%Y.%m.%d')\n",
    "    #to에서 하루를 더 더한 날짜를 20XX.XX.XX 형태로 나타내어 반복문에서 원래 now 일자까지의 기사 개수가 나오도록 조정함.\n",
    "    #20XX.XX.XX로 형태를 조정하는 이유: naver에서 리퀘스트 할때 이 형태여야 하기 때문이다.\n",
    "    \n",
    "    from_Date = from_.strftime('%Y.%m.%d')\n",
    "    #기사 개수를 보길 원하는 시작 날짜을 20XX.XX.XX 형태로 나타냄.\n",
    "\n",
    "\n",
    "    while from_Date != to_Date:\n",
    "        #내가 설정한 과거 일자부터 현재까지의 기사 개수를 나타냄.\n",
    "\n",
    "        print(from_Date)\n",
    "\n",
    "        search_Date = from_Date\n",
    "        #네이버에 리퀘스트를 요청할 날짜를 pastDate로 설정함.\n",
    "\n",
    "        address = 'https://search.naver.com/search.naver?where=news' + '&query={}&pd=3&ds={}&de={}'.format(search_Sentence,search_Date,search_Date)\n",
    "        #네이버에서 특정 일자의 특정 단어가 들어간 기사의 개수를 찾기 위한 URL 양식\n",
    "        print(address)\n",
    "\n",
    "        req = requests.get(address)\n",
    "        #특정 address에 해당하는 사이트의 모든 정보를 req에 저장함\n",
    "\n",
    "        html = req.text\n",
    "        #req에 해당하는 정보를 text 파일 형태로 html에 저장함\n",
    "\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        #html에 저장된 내용을 html.parser'이라는 도구를 사용해 파싱함.\n",
    "\n",
    "        all_news_information = BeautifulSoup(html, 'html.parser')\n",
    "        #html에 저장된 내용을 html.parser'이라는 도구를 사용해 파싱함.\n",
    "    \n",
    "        news_number_info = all_news_information.select('#main_pack > div.news.mynews.section._prs_nws > div.section_head > div.title_desc.all_my > span')\n",
    "        #Copy selecter에 의해 복사된 태그 사이에 들어가는 정보만 가져옴, 여기서는 뉴스 개수만 가져오기 위함임.\n",
    "    \n",
    "        if not news_number_info:\n",
    "            print('0건')\n",
    "            print('----------------------------')\n",
    "            print('')\n",
    "\n",
    "        else:\n",
    "\n",
    "            news_number_info = news_number_info[0]\n",
    "            news_number_info = str(news_number_info)\n",
    "            change_to_news_number = news_number_info.split(' / ')\n",
    "            change_to_news_number = change_to_news_number[1]\n",
    "            change_to_news_number_2 = change_to_news_number.split('<')\n",
    "            only_news_number = change_to_news_number_2[0]\n",
    "            #데이터들 중에서 온전히 뉴스 개수만 가져오기 위한 과정\n",
    "\n",
    "\n",
    "            print(only_news_number)\n",
    "            print('----------------------------')\n",
    "            print('')\n",
    "\n",
    "        from_ = from_ + datetime.timedelta(days=1)\n",
    "        from_Date = from_.strftime('%Y.%m.%d')\n",
    "        \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
